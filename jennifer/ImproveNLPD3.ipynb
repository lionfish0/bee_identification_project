{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the NLPD using the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerXtest = pd.read_csv('Weights2 - 6th  layer/Uk_weights2_X_test.csv',header=None).to_numpy()\n",
    "layerYtest = pd.read_csv('Weights2 - 6th  layer/Uk_weights2_Y_test.csv',header=None).to_numpy()\n",
    "layerXtrain = pd.read_csv('Weights2 - 6th  layer/Uk_weights2_X_validation.csv',header=None).to_numpy()\n",
    "layerYtrain = pd.read_csv('Weights2 - 6th  layer/Uk_weights2_Y_validation.csv',header=None).to_numpy()\n",
    "smXtest = pd.read_csv('Weights2 - Softmax/Uk_weights2_X_test.csv',header=None).to_numpy()\n",
    "smYtest = pd.read_csv('Weights2 - Softmax/Uk_weights2_Y_test.csv',header=None).to_numpy()\n",
    "smXtrain = pd.read_csv('Weights2 - Softmax/Uk_weights2_X_validation.csv',header=None).to_numpy()\n",
    "smYtrain = pd.read_csv('Weights2 - Softmax/Uk_weights2_Y_validation.csv',header=None).to_numpy()\n",
    "\n",
    "#we'll include the training data too...\n",
    "smXtrain2 = pd.read_csv('Weights2 - Softmax/Uk_weights2_X_train.csv',header=None).to_numpy()\n",
    "smYtrain2 = pd.read_csv('Weights2 - Softmax/Uk_weights2_Y_train.csv',header=None).to_numpy()\n",
    "smXtrain = np.r_[smXtrain,smXtrain2]\n",
    "smYtrain = np.r_[smYtrain,smYtrain2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we want to just use the original test data and split it into training + test.\n",
    "orgtest_smXtrain = smXtest[::2,:]\n",
    "orgtest_smXtest = smXtest[1::2,:]\n",
    "orgtest_smYtrain = smYtest[::2,:]\n",
    "orgtest_smYtest = smYtest[1::2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLPD(actual,predicted):\n",
    "    \"\"\"(normalised) Negative Log Predictive Density\n",
    "    Definition of Negative Log Predictive Density (NLPD):\n",
    "    $$L = -\\frac{1}{n} \\sum_{i=1}^n \\log p(y_i=t_i|\\mathbf{x}_i)$$\n",
    "    See http://mlg.eng.cam.ac.uk/pub/pdf/QuiRasSinetal06.pdf, page 13.\n",
    "    \"This loss penalizes both over and under-confident predictions.\"\n",
    "    but\n",
    "    \"The NLPD loss favours conservative models, that is models that tend to be under-confident\n",
    "    rather than over-confident. This is illustrated in Fig. 7, and canbe deduced from the fact that\n",
    "    logarithms are being used. An interesting way ofusing the NLPD is to give it relative to the NLPD\n",
    "    of a predictor that ignoresthe inputs and always predicts the same Gaussian predictive distribution,\n",
    "    withmean and variance the empirical mean and variance of the training data. Thisrelative NLPD\n",
    "    translates into a gain of information with respect to the simpleGaussian predictor described.\"\n",
    "    \n",
    "    actual = 2d array of indices (Nx1), values are from 0 to D-1.\n",
    "    predicted = 2d array of probabilities (NxD) sum to one along rows.\n",
    "    \"\"\"\n",
    "    assert np.max(np.abs(np.sum(predicted,1)-1))<1e-5 #confirmed every row roughly adds up to one\n",
    "    assert 0<=np.max(predicted)<=1\n",
    "    return -np.sum(np.log(np.take_along_axis(predicted,actual.astype(int),axis=1)))\n",
    "\n",
    "def MNLPD(actual,predicted):\n",
    "    \"\"\"Mean of the NLPD\"\"\"\n",
    "    assert np.max(np.abs(np.sum(predicted,1)-1))<1e-5 #confirmed every row roughly adds up to one\n",
    "    assert 0<=np.max(predicted)<=1\n",
    "    return -np.mean(np.log(np.take_along_axis(predicted,actual.astype(int),axis=1)))\n",
    "def GMNLPD(actual,predicted):\n",
    "    \"\"\"Grouped Mean NLPD\n",
    "    This finds the mean of the MNLPDs for all the separate actual groups.\n",
    "    It means that if one large group is well classified it won't swamp the results from\n",
    "    the smaller groups\"\"\"\n",
    "    assert np.max(np.abs(np.sum(predicted,1)-1))<1e-5 #confirmed every row roughly adds up to one\n",
    "    assert 0<=np.max(predicted)<=1\n",
    "    res = []\n",
    "    for i in np.unique(actual):\n",
    "        res.append(MNLPD(actual[actual[:,0]==i],predicted[actual[:,0]==i,:]))\n",
    "    return np.mean(res)\n",
    "\n",
    "\n",
    "def build_conf(smXtrain,smYtrain):\n",
    "    preds = np.argmax(smXtrain,1)\n",
    "    conf = np.zeros([19,19])\n",
    "    for p,a in zip(preds,smYtrain[:,0].astype(int)):\n",
    "        conf[p,a]+=1.0\n",
    "    conf+=0.01\n",
    "    conf = (conf.T/np.sum(conf,1)).T\n",
    "    return conf\n",
    "\n",
    "def getprobs(conf,smXtest):\n",
    "    newprobs = np.zeros_like(smXtest)\n",
    "    preds = np.argmax(smXtest,1)\n",
    "    for i,p in enumerate(preds):\n",
    "        newprobs[i,:] = conf[p,:]\n",
    "    return newprobs\n",
    "\n",
    "def build_prob_conf(smXtrain,smYtrain,normalise_by_predclass=False):\n",
    "    \"\"\"\n",
    "    set normalise_by_predclass true for best GMNLPD score.\n",
    "    \"\"\"\n",
    "    conf = np.zeros([19,19])\n",
    "    for x,a in zip(smXtrain,smYtrain[:,0].astype(int)):\n",
    "        conf[:,a]+=x\n",
    "\n",
    "    #disable this line for NLPD, enable for GMNLPD.\n",
    "    if normalise_by_predclass:\n",
    "        conf = (conf/np.sum(conf,0))\n",
    "    conf = (conf.T/np.sum(conf,1)).T\n",
    "    conf+=np.eye(19)*1\n",
    "    conf = (conf.T/np.sum(conf,1)).T\n",
    "    return conf\n",
    "\n",
    "def regularise(mat):\n",
    "    tempX = mat+0.01\n",
    "    tempX=(tempX.T/np.sum(tempX,1)).T\n",
    "    return tempX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we look at the NLPD and also the Grouped Mean NLPD 'GMNLPD' this latter measure weights the different species classes equally (and finds the mean per sample)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Softmax solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Neural network\n",
      "(using original training data) NLPD:   5655.7, GMNLPD:  1.566\n",
      "(using original test data)     NLPD:   2791.4, GMNLPD:  1.361\n",
      "\n",
      "Trying to regularise the CNN softmax solution to see if it helps (as we regularise our EUE method, so it's fair to try it here too).\n",
      "\n",
      "Regularised Deep Neural network\n",
      "(using original training data) NLPD:   5764.6, GMNLPD:  1.302\n",
      "(using original test data)     NLPD:   2877.5, GMNLPD:  1.201\n"
     ]
    }
   ],
   "source": [
    "print(\"Deep Neural network\")\n",
    "print(\"(using original training data) NLPD: %8.1f, GMNLPD: %6.3f\" % (NLPD(smYtest,smXtest),GMNLPD(smYtest,smXtest)))\n",
    "print(\"(using original test data)     NLPD: %8.1f, GMNLPD: %6.3f\" % (NLPD(orgtest_smYtest,orgtest_smXtest),GMNLPD(orgtest_smYtest,orgtest_smXtest)))\n",
    "print(\"\\nTrying to regularise the CNN softmax solution to see if it helps (as we regularise our EUE method, so it's fair to try it here too).\\n\")\n",
    "\n",
    "print(\"Regularised Deep Neural network\")\n",
    "tempX = regularise(smXtest)\n",
    "print(\"(using original training data) NLPD: %8.1f, GMNLPD: %6.3f\" % (NLPD(smYtest,tempX),GMNLPD(smYtest,tempX)))\n",
    "tempX = regularise(orgtest_smXtest)\n",
    "print(\"(using original test data)     NLPD: %8.1f, GMNLPD: %6.3f\" % (NLPD(orgtest_smYtest,tempX),GMNLPD(orgtest_smYtest,tempX)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularising improves the GMNLPD but not the NLPD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Uncertainty Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using just the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(using original training data) NLPD:   5415.5, GMNLPD:  2.333\n",
      "(using origial test data) NLPD:        2744.8, GMNLPD:  2.275\n"
     ]
    }
   ],
   "source": [
    "conf = build_conf(smXtrain,smYtrain)\n",
    "newprobs = getprobs(conf,smXtest)\n",
    "print(\"(using original training data) NLPD: %8.1f, GMNLPD: %6.3f\" % (NLPD(smYtest,newprobs),GMNLPD(smYtest,newprobs)))\n",
    "conf = build_conf(orgtest_smXtrain,orgtest_smYtrain)\n",
    "newprobs = getprobs(conf,orgtest_smXtest)\n",
    "print(\"(using origial test data) NLPD:      %8.1f, GMNLPD: %6.3f\" % (NLPD(orgtest_smYtest,newprobs),GMNLPD(orgtest_smYtest,newprobs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the probabilities to build a sort of \"probability confusion matrix\" (this might have a name)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(using original training data) NLPD:   4473.8, GMNLPD:  1.457\n",
      "(using origial test data) NLPD:        2228.1, GMNLPD:  1.313\n"
     ]
    }
   ],
   "source": [
    "conf = build_prob_conf(smXtrain,smYtrain)\n",
    "newprobs = smXtest@conf\n",
    "print(\"(using original training data) NLPD: %8.1f, GMNLPD: %6.3f\" % (NLPD(smYtest,newprobs),GMNLPD(smYtest,newprobs)))\n",
    "conf = build_prob_conf(orgtest_smXtrain,orgtest_smYtrain)\n",
    "newprobs = orgtest_smXtest@conf\n",
    "print(\"(using origial test data) NLPD:      %8.1f, GMNLPD: %6.3f\" % (NLPD(orgtest_smYtest,newprobs),GMNLPD(orgtest_smYtest,newprobs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can minimise the NLPD by simply transforming by the confusion probability matrix.\n",
    "The GMNLPD is minimised by normalising by the number of observations (this obviously harms the\n",
    "overall NLPD but the GMNLPD is interested in the average over each actual species)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalising the probability confusion matrix\n",
      "(using original training data) NLPD:   5321.5, GMNLPD:  1.168\n",
      "(using origial test data) NLPD:        2857.1, GMNLPD:  1.092\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalising the probability confusion matrix\")\n",
    "conf = build_prob_conf(smXtrain,smYtrain,normalise_by_predclass=True)\n",
    "newprobs = smXtest@conf\n",
    "print(\"(using original training data) NLPD: %8.1f, GMNLPD: %6.3f\" % (NLPD(smYtest,newprobs),GMNLPD(smYtest,newprobs)))\n",
    "conf = build_prob_conf(orgtest_smXtrain,orgtest_smYtrain,normalise_by_predclass=True)\n",
    "newprobs = orgtest_smXtest@conf\n",
    "print(\"(using origial test data) NLPD:      %8.1f, GMNLPD: %6.3f\" % (NLPD(orgtest_smYtest,newprobs),GMNLPD(orgtest_smYtest,newprobs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if you want the best NLPD don't normalise by number in each class - in both datasets this results in a 20\\% reduction in the NLPD. If you are concerned about the NLPD with equal weighting by class then normalise, this approach led to a 10\\% reduction in the GMNLPD in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.791158267020336, 0.7982801863131495, 0.9092422980849293, 0.8970814132104454)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4474/5655,2228/2791,1.092/1.201,1.168/1.302"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
